# ==============================================================================
# AIRFLOW REMOTE WORKER (KG5090)
# ==============================================================================

# Base image (MUST match prod Airflow version)
AIRFLOW_IMAGE=apache/airflow:2.10.2-python3.11

# Executor (MUST be CeleryExecutor for remote workers)
AIRFLOW__CORE__EXECUTOR=CeleryExecutor

# ------------------------------------------------------------------------------
# Control Plane Connectivity (same as prod kg-pc)
# ------------------------------------------------------------------------------
# Postgres via Tailscale (kg-pc.tail5c5268.ts.net)
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@kg-pc.tail5c5268.ts.net:5432/airflow

# SQLAlchemy pool settings (prevents connection exhaustion with high concurrency)
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=5
AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=10
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE=1800
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_PRE_PING=True

# Celery Broker (Local Redis on kg-pc - DB index /1, no SSL)
AIRFLOW__CELERY__BROKER_URL=redis://:fidget_redis_2025@kg-pc.tail5c5268.ts.net:6379/1

# Result Backend (same Postgres)
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@kg-pc.tail5c5268.ts.net:5432/airflow

# Fernet key (MUST match prod)
AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here

# ------------------------------------------------------------------------------
# Worker Behavior
# ------------------------------------------------------------------------------
# Only listen to massive_trades queue (fetch_and_upsert_trades_batch tasks)
WORKER_QUEUES=massive_trades
WORKER_CONCURRENCY=128
WORKER_HOSTNAME=kg5090@%h
# WORKER_AUTOSCALE=16,4

# ------------------------------------------------------------------------------
# Celery Serialization (MUST match prod)
# ------------------------------------------------------------------------------
AIRFLOW__CELERY__TASK_SERIALIZER=pickle
AIRFLOW__CELERY__RESULT_SERIALIZER=pickle
AIRFLOW__CELERY__ACCEPT_CONTENT=["pickle","json"]
AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER=1
AIRFLOW__CELERY__BROKER_CONNECTION_RETRY_ON_STARTUP=True

# ------------------------------------------------------------------------------
# DAG/Runtime Secrets (as needed by your tasks)
# ------------------------------------------------------------------------------
OPENAI_API_KEY=sk-...
TIINGO_TOKEN=...
TAVILY_API_KEY=tvly-...
MONGODB_URI=mongodb://...
REDDIT_CLIENT_ID=...
REDDIT_CLIENT_SECRET=...
REDDIT_USER_AGENT=...
UNUSUALWHALES_API_KEY=...
MASSIVE_API_KEY=...
TWITTER_API_KEY=...

# Redis URL (for jobs that use Redis directly, no SSL)
REDIS_URL=redis://:fidget_redis_2025@kg-pc.tail5c5268.ts.net:6379/1

# ------------------------------------------------------------------------------
# ClickHouse connection (via Tailscale to kg-pc)
# ------------------------------------------------------------------------------
CLICKHOUSE_HOST=kg-pc.tail5c5268.ts.net
CLICKHOUSE_PORT=8123
CLICKHOUSE_USER=fidget_admin
CLICKHOUSE_PASSWORD=...
